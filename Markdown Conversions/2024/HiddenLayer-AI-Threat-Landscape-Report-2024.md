AI THREAT2024LANDSCAPEREPORTUNDERSTANDING THE EVOLVING
CYBERSECURITY ENVIRONMENTTABLE OF CONTENTSForewordSurvey Insights at a GlanceAdversarial AI Over TimePart 1: Risks Related to the Use of AI
Harmful Content Creation
Deepfakes
Data Privacy and Leakage
Copyright Violation
Accuracy and Bias Issues
Other Ethical \& Societal IssuesPart 2: Risks Faced by AI\-based Systems 
Adversarial Machine Learning Attacks
Attacks Specic to Generative AI
Supply Chain Attacks
Threat Actors and Attack VectorsPart 3: Advancements in Security for AI
Offensive Security Tooling for AI
Defensive Frameworks for AI
Red Teaming and Risk Assessment
Policies and RegulationsPart 4: Predictions and RecommendationsResourcesAbout HiddenLayer02030608
09
09
10
10
11
1213
13
20
22
2628
28
30
34
353640431AI THREAT LANDSCAPE 2024FOREWORDHumanity has entered an unprecedented technological evolution. No mission, organization, job, or personon the planet will go unimpacted by articial intelligence this year. Revolutionizing every data\-drivenopportunity, AI has the potential to bring on a new era of prosperity, allowing the quality of life to reachunimaginable heights. Like any new groundbreaking technology, the potential for greatness is paralleledonly by the inherent risk. It is critical that we do not allow ourselves to tunnel solely on harvesting thebenets of AI without responsibly mitigating those risks. Make no mistake, for all the distrust of the blackbox nature of AI and the doom and gloom rhetoric of world domination, the greatest risk associated withAI for the foreseeable future is bad people.Articial intelligence is, by a wide margin, the most vulnerable technology ever to be deployed inproduction systems. Its vulnerable at a code level, during training and development, post\-deployment,over networks, via generative outputs, and more. We do not need to look far into the traditional cyberthreat landscape to understand todays adversarial AI attacks and predict their near\-term patterns.In this report, we shed light on these vulnerabilities and how they impact commercial and federalorganizations today. We provide insights from a survey of IT security and data science leaders navigatingthese challenges. We share predictions driven by data from HiddenLayers experiences securing AI inenterprise environments. Lastly, we reveal cutting\-edge advancements in security controls for AI in all itsforms.As we navigate an AI\-driven era, let this report serve as a resource to understand and implement securityfor AI. Whether youre a developer, data scientist, or security professional, we invite you to join us insecuring AI for a safer future.We are incredibly excited to present to you the rst\-ever HiddenLayer AI Threat Landscape Report.Tito
CEO \& Co\-Founder
(Unassisted by LLMs)2SECURITY FOR AISURVEY INSIGHTS
AT A GLANCEIts important to know that AI is not some invincible newtechnology, but rather, a technology extremely vulnerable tocyber threats just like many others that came before it. Themotivations for attacking AI are what you would expect. Theyrange from nancial gain to manipulating public opinion togaining competitive advantage. While industries are reapingthe benets of increased efficiency and innovation thanks toAI, there is still the concerning reality that expanding the useof AI causes a signicant increase in security risks.A survey of 150 IT security leaders commissioned by HiddenLayer conrms this concern. As the below results show, theindustry is working hard to accelerate AI adoption without having the proper security measures in place.Pervasive Use of AIOn average, companies
have a staggering1,689AI models in 
production.98%of IT leaders consider at 
least some oftheir AI 
models crucial to their 
business success.83%state that AI usage is 
prevalent across all 
teams within their 
organizations.3SECURITY FOR AI \- SURVEY INSIGHTS AT A GLANCEChallenges in Securing AI61%of IT leaders acknowledge shadow AI 
(solutions that are not officially known or 
under the control of the IT department) as a 
problem within their organizations.89%express concern about 
security vulnerabilities 
associated with integrating 
third\-party AIs.75%believe third\-party AI 
integrations pose a 
greater risk than
existing threats.Budgets and Priorities97% of IT leaders prioritizesecuring AI92%are still developing a 
comprehensive plan for 
this emerging threat.94% allocated budgets forAI security in 2024, but only61% are highly condentin their allocation.Sources of AI BreachesAccording to IT leaders, the top sources ofAI breaches include:criminal hacking individuals or groupsthird\-party service providersautomated botnetscompetitorsSecurity Breaches Looming77%of companies reported 
identifying breaches to 
their AI in the past year. The 
remaining were uncertain 
whether their AI models 
had seen an attack.4SECURITY FOR AI \- SURVEY INSIGHTS AT A GLANCESecurity MeasuresCommon measures to secure AI involve30%of IT leaders have deployed a manual 
defense for adversarial attacks on 
their existing AI, while justbuilding relationships with AI and security teamsscanning and auditing AI models14%are planning and testing for such 
attacks.and determining the origin source of AI models.Collaboration and Concerns83%of IT leaders collaborate 
with external
cybersecurity rms to 
enhance AI security.58%express doubts that the 
security protocols theyve 
implemented can keep 
pace with evolving threats.Future Planning96%of IT leaders expressed that 
their AI projects are critical 
or important to revenue 
generation over the next 18 
months.30%Only 30% have deployed technology 
for model theftjacking, with20%planning and testing for this threat.Seeking Technological Solutions98%of IT leaders are actively 
seeking technological 
solutions to enhance the 
security of AI and machine 
learning models.92%of companies are building 
their own models to improve 
business operations.5ADVERSARIALAI OVER TIMESelected Offensive Milestones\+Selected Defensive Milestones\+AI Milestones200220042006201220142015201620162017201820182019Adoption of ML\-based spam detection lters using Naive Bayes algorithmFirst evasion techniques in linear spam lters by inserting good wordsFirst paper outlining taxonomy of attacks against MLFirst gradient\-based poisoning attack against non\-linear algorithmsFirst demonstrated attack against deep neural networksOpenAI foundedCrowd\-sourced poisoning of Microsofts Tay chatbotHugging Face launched as a chatbot serviceFirst demonstrated black\-box attack against machine learningIntroduction of Boundary AttackFull model extraction attacks: KnockOffNets,CopycatCNNIntroduction of One Pixel andattacksHopSkipJump6ADVERSARIAL AI OVER TIME201920192019202020212021202220222022202220222022202220222022202220232023202320232023202320232023Cylance endpoint bypass released \- Cylance, I kill youSingapore's Model AI Governance FrameworkNIST IR Adversarial Machine Learning:A Taxonomy andTerminology of Attacks and MitigationHugging Face introduces the concept of "model cards" and "datasets"First black\-box neural payload injection techniqueMITRE ATLAS releasedEU AI ActFirst prompt injection attacks against LLMs disclosedPotential supply chain attack with ransomware embedded into AI modelCanadas Articial Intelligence and Data Act (AIDA)Hugging Face has 50K models, 5K datasets, and 5K demosUS Blueprint for an AI Bill of RightsGartner TRISMOpenAI launches ChatGPTtorchtriton \- malicious PyTorch dependency found on PyPIFirst ITW hijacked models containing reverse\-shells and stagersNIST AI Risk Management Framework (AI RMF)First open\-source replicas of closed\-source models (Alpaca, OpenLLaMA)GoogleSecure AI Framework (SAIF)PoisonGPT \- a demonstration of LLM poisoningHugging Face valued at $4\.5 billion in latest round of fundingNightshade \- a tool for poisoning text\-to\-image modelsUS White House issues an executive order on the safe, secure, andtrustworthy development and use of articial intelligenceHugging Face has 520k models and 112k datasets.7PART 1:RISKS RELATED TO
THE USE OF AILike with any other life\-changing technology, articialintelligence is a double\-edged sword. Although itsalready starting to have a massively positive impacton our lives and workows, it also has tremendouspotential to cause serious harm, especially if usedcarelessly or with overt malicious purposes.There are plenty of ways in which adversaries \- such as criminals, terrorists, cyber threat actors, foul\-playing competitors,and repressive nation\-states \- can utilize AI to their advantage. There are also numerous obscure risks related to thelegitimate use of this technology.Generative AI is especially vulnerable to abuse.
It can be:manipulated to give biased, inaccurate, or 
harmful informationused to create harmful content, such as 
malware, phishing, and propagandaPrivacy is also an issue when it comes to the informationwe share with AI\-based tools. Data leakage can causesignicant legal issues for businesses and institutions.In addition, because of code generation tools,vulnerabilities could be introduced into theused to develop deepfake images, audio 
and videosoftware \- intentionally, by poisoning the datasets, orunintentionally, by training the models on alreadyleveraged by any malicious activity to provide
access to dangerous or illegal informationvulnerable code. All this is on top of copyright violationsand various ethical and societal concerns that leadingindustry experts have repeatedly voiced.8PART 1:RISKS RELATED TO THE USE OF AIHarmful Content CreationThe cybercrime business has skyrocketed. Everythingfrom easily accessible dark web marketplaces toready\-to\-use attack toolkits and Ransomware\-as\-a\-Ser\-vice leveraging practically untraceable cryptocurrencies,have helped cybercriminals thrive as law enforcementstruggles to track them down.As if this wasnt badenough, generative AI enables instant and effortlessaccess to a world of devious attack scenarios whileproviding elaborate phishing and malware for anyonewho dares to ask for it. AI chatbots can also access illegalinformation that could result in physical threatsMalicious users could, for example, evade the protectivelters of a chatbot and trick it into providing recipes formaking explosives.OpenAI and Microsoft have recently unveiled themany ways in which state\-affiliated threat actorstried to abuse their AI solutions to aid maliciouscampaigns. Adversaries with links to North Korea,Iran, Russia and China were found to use largelanguage models for assistance with activitiessuch as scripting, social engineering, vulnerabilityresearch, post\-exploitation techniques, detectionevasion, and military reconnaissance.While the most widely used generative AI solutions striveto implement strong lters and content restrictions, mosthave been proven relatively easy to bypass. Moreover,open\-source AI models can be ne\-tuned to work withoutany restrictions whatsoever. Such models could be keptprivate to the adversaries or provided to the broaderpublic on the dark web. The security community still needsto devise a workable solution to the complicated problemof accessing illegaldangerous content via AI chatbots.DeepfakesAnother obvious concern is the creation of very authen\-tic\-looking deepfake images, audio, and video. These couldbe used to steal money, extract sensitive information, ruinpersonal reputations, and spread misinformation.In one of the biggest deepfake scams to date,adversaries were able to defraud a multinationalcorporation of $25 million. The nancial workerwho approved the transfer had previously attend\-ed a video conference call with what seemed to bethe company's CFO, as well as a number of othercolleagues the employee recognized. These allturned out to be deepfake videos.Scammers have for years been able to mislead people.Even the least sophisticated error\-ridden messages andcalls usually claim a number of victims. The emergence ofdeepfakes brings this problem to a completely new level,where even a seasoned cybersecurity expert will havehard time distinguishing truth from falsehood. It's not justmoney and reputation that is at stake here. Deepfakes canbe used to disrupt political campaigns, rig democraticelections, manipulate societies, and stir unrest. Democra\-cy and social order can greatly suffer if sufficientmeasures are not timely implemented.9PART 1:RISKS RELATED TO THE USE OF AIData Privacy and LeakageCopyright ViolationWhen a new, exciting technology that makes peoplesThe rapid, large\-scale incorporation of generative AI willlives easier comes to market, its hard not to dive in andlikely spur a variety of legal issues. For now, the mainreap the benets immediately especially if its free.concern is the unauthorized use of copyrighted materialsBut we should all be aware that although were notin training datasets and models, which leads to producingshelling out money, there is still a cost: our data.plagiarized output.Guidelines for protecting privacy always lag behind theThe models behind generative AI solutions are typicallyadoption of new technologies. Too often, the implicationstrained on swaths of publicly available data, some ofof privacy breaches become clear only after the initialwhich are protected by copyright laws. The generatedfuror dies down. We saw this with social networks and arecontent is merely a mix of things published somewherealready seeing it happen with generative AI.(text, pictures, video, or audio) and included in thetraining dataset. The problem is that generative AI cantFor example, the terms and conditions agreement for anydistinguish between inspiration and plagiarism. It oftenAI\-based service should state how the service providergives outputs too close to the creations it was traineduses our request prompts. However, these are oftenon without crediting the original works authors.intentionally lengthy texts written in difficult language. IfThis can result in serious copyright violations.you dont want to spend hours deciphering the ne print,its best to assume that every request made to the modelThere is also the question of consent. Currently, no lawsis logged, stored, and processed in one way or another. Atprevent service providers from training their models ona minimum, expect that your inputs are fed into theany kind of data as long as its legal and public. This istraining dataset and, therefore, could be accidentallyhow a generative AI can write a poem or create an imageleaked in outputs for other requests.in a specic authors style. Understandably, most writersand artists do not appreciate their work being used insuch a way.In March 2023, Samsung experienced a seriousleakage of intellectual property, where employeeswere found to be pasting portions of proprietarysource code into ChatGPT. This resulted in acompany\-wide ban on the usage of AI chatbots.Moreover, many providers might feel tempted to proton the side and sell the input data to research rms,advertisers, or any other interested third party.10PART 1:RISKS RELATED TO THE USE OF AIStability AI, which provides one of the mostpopular text\-to\-image generators called StableDiffusion, is facing multiple lawsuits for wrongfullyusing copyright\-protected images to train theirmodels. One of these lawsuits, brought on byGetty Images, alleges that Stability AI utilizedmillions of copyrighted images and their metadatawithout obtaining permission from Getty oroffering any compensation. Several artists alsoled class\-action suits against both Stability AIand its rival, Midjourney, pointing out that imagesgenerated in the style of a particular authordirectly compete with the author's own work.Accuracy and Bias IssuesThe old saying states that AI models are only as good astheir training dataset. But large generative AI models aretrained on terabytes of data, in most cases indiscriminate\-However, more often than not, the training processinvolves large amounts of historical information collectedover the past decades. Such data tends to be veryunder\-representative of marginalized groups. Because ofthis, resulting models will be inherently biased towardsthings they nd more common in their training datasets.The built\-in bias of AI can be easily seen in the case of textgeneration and image generation models. These modelsoften follow gender, age, and skin color stereotypes thatare deemed inappropriate and harmful in modern society.The damage can be very serious if a biased model isimplemented in such settings as healthcare, nance, orhuman resources.In 2019, the AI algorithm used in the U.S.healthcare system was found to be racially biasedwhich resulted in black patients receiving lowerrisk scores and were less often identiedfor extra care.ly scraped from the Internet, making careful vetting of theEven if the dataset contains unbiased and accurate infor\-training set impossible. This causes problems concerningmation, an AI algorithm does not always get it right andthe accuracy, fairness, and general sanity of the model, asmight sometimes arrive at bizarrely incorrect conclusions.well as the possibility of data privacy breaches if themodel is accidentally trained on sensitive data. Moreover,the rise of online learning, where user input is continuous\-ly fed into the training process, makes AI solutions proneto bias, misinformation, and intentional poisoning.AI algorithms have no notion of fairness on their own,so they need to be trained on a well\-balanced and fullyrepresentative dataset in order to avoid any kind ofdiscrimination.Metas short\-lived Galactica model was trainedon millions of scientic articles, textbooks, andwebsites. Despite the training set likely beingthoroughly vetted, the model was servingfalsehoods and pseudo\-scientic babble in amatter of hours, making up citations that neverexisted and inventing papers written byimaginary authors.11PART 1:RISKS RELATED TO THE USE OF AIThese are called hallucinations and are an intrinsicattribute of current AI technology. By design, AI cannotdistinguish between reality and ction, so if the trainingdataset contains a mix of both, chances are the AI will attimes respond with ction.Other Ethical \& Societal IssuesBesides biased and inaccurate information, a generative AImodel can also give advice that appears technically sanebut can prove harmful in certain circumstances or whenthe context is missing or misunderstood. This is especiallytrue in so\-called emotional AI machine learning applica\-tions designed to recognize human emotions. Suchapplications have been used for some time, mainly inmarket trend predictions, but are increasingly adopted inhuman resources and counseling.Given the probabilistic nature of the AI models andthe often lack of necessary context, this can be quitedangerous. Privacy watchdogs now warn against usingemotional AI in any professional setting.The ability of AI to almost perfectly mimic human behaviorcan prove very dangerous. Some people might be com\-pelled to believe the AI bot's hallucinations or evenconclude that it is sentient; others might feel intimidatedor hurt by its emotionally charged responses. In somecircumstances, people could be manipulated to give awaysensitive data or act in a harmful way. This is just the tipof the iceberg.It was recently revealed that a would\-be assassin,who was arrested on his way to kill the BritishQueen with a crossbow in December 2021, was infact encouraged to do so by an AI chatbot.Prior to the attack, the man created an articial"girlfriend" on Replika, a platform that offerspersonalized and empathetic AI companions. Heexchanged thousands of messages with thechatbot, many of them discussing the murderousplan. The bot responses were in support of theplan and bolstered the condence of the attacker.Used with malicious intent, AI chatbots can becomevery effective tools in misinformation and manipulation especially if people are led to believe that they areinteracting with fellow humans. Add voice and videosynthesis to the mix, and we get something far moreterrifying than Twitter bots and fake Facebook accounts.If highly personalized and trained on specially crafteddatasets, such bots could steal the identities of real people.With the rapid adaptation of generative AI, there is asubstantial prospect that AI creations will dominate theweb in a couple of years. At the moment, disclosing the useof AI in producing content is not a legal requirement, so wecan expect that there are many more AI\-generated texts onthe web than it might seem on the surface. The speed atwhich chatbots can produce data, coupled with easyaccess for everyone in the world, means that we mightsoon become overwhelmed with dubious\-quality AI\-gener\-ated material. Moreover, suppose we keep training themodels on online data. In that case, they will eventually befed their own creations in an ever\-lasting quality\-degradingloop, turning the Dead Internet theory into reality.12PART 2:RISKS FACED BYAI\-BASED SYSTEMSTheres a lot of conversation about the safe andethical use of AI\-powered tools; however, the securityand safety of AI systems themselves are still oftenoverlooked. Its vital to remember that, like with anyother ubiquitous technology, AI\-based solutions canbe abused by attackers, resulting in disruption,nancial loss, reputational harm, or even risk tohuman health and life.Three major types of attacks on AI:Adversarial Machine Learning Attacks \-attacks against AI algorithms, aimed to alterAIs behavior, evade AI\-based detection, orsteal the underlying technologyGenerative AI System Attacks \- attacksagainst AIs lters and restrictions, intended togenerate content deemed harmful or illegalAdversarial Machine LearningAttacksTo help you understand adversarial machine learningattacks, lets rst go over some basic terminology.Articial intelligence is the general term comprisingany system that mimics human intelligence.Machine Learning is the technology that enables AIto learn and improve its predictions.Machine Learning Models are the decision\-makingSupply Chain Attacks \- attacks against MLsystems that lie at the core of most modern AI\-based. Itartifacts and platforms, with the intention ofanalyzes the input, such as a picture, a text prompt, or aarbitrary code execution and delivery ofbinary le, and makes a prediction based on thetraditional malwareinformation it has learned from in the past.Model Training involves feeding vast amounts ofrelevant data into a machine learning algorithm toproduce a trained model, which can then be deployedinto production and made available for users to querythrough an interface or an API.13PART 2: RISKS FACED BY AI\-BASED SYSTEMSAdversarial attacks against machine learning usually aim to do three things:Alter the models behavior, for example, to make it biased, inaccurate, or maliciousBypass or evade the model, for example, to trigger incorrect classication or avoid detectionReplicate the model or data used to train it, stealing the intellectual property.Lets look at some of the most popular machine learning attacks today.Data PoisoningModel training is one of the crucial phases in building anData poisoning attacks aim to modify the model's behavior.AI\-based solution. During this stage, the model learns howThe goal is to make the predictions biased, inaccurate, orto behave based on inputs from the training dataset.otherwise manipulated to serve the attackers purpose.Any malicious interference in the learning process canAttackers can perform data poisoning in two ways: bysignicantly impact the reliability of the resulting model.modifying entries in the existing dataset (for example,changing features or ipping labels) or injecting thedataset with a new, specially doctored portion of data.Attacks Against AI \- Data PoisoningTraining
DataTraining
ProcessTrained
ModelDecision
ProcessInputPredictionAI solutions that are most prone to this type of attack usedata, and predictions from a model trained on inaccuratecontinuous learning. This is where the model is constantlydata will always be biased or incorrect. One or few poisonedretrained on new user\-supplied data. Because the usersrequests will hardly make a difference. Still, adversaries caninput is often not carefully validated and sanitized, antry to manipulate the public to interact with the model in aadversary can craft specic inputs to sway the model.specic way or use botnets to amplify the amount ofA model is only as good as its trainingpoisoned input sent to the model.14PART 2: RISKS FACED BY AI\-BASED SYSTEMSSystems that often make use of online training orData Poisoning in the Wildcontinuous\-learning models and, therefore, are susceptibleto data poisoning attacks include:Chatbots and digital assistantsOne of the rst widely publicized examples of datapoisoning concerned Microsoft's early chatbot called Tay.Continuously trained on user\-provided input, Tay launchedon Twitter in March 2016 \- only to be shut down after a mere16 hours of existence. In this short timeframe, usersText auto\-complete toolsmanaged to sway the bot to become rude and racist andTrend prediction and recommendation systemsproduce biased and harmful output. Although it was not acoordinated attack, Microsoft suffered some reputationalSpam lters and anti\-malware solutionsdamage just because of unintended trolling and was evenIntrusion detection systemsFinancial fraud preventionMedical diagnostic toolsMany modern ML solutions opt for a distributed learningmethod called federated learning, where the trainingdataset is scattered amongst several independent devices.During federated learning, the ML model is downloaded andtrained locally on each participating edge device. Theupdates are pushed to the central server or shared directlythreatened with legal action.More sophisticated attempts at data poisoning couldpotentially have a devastating impact. Worse, pre\-trainedmodels are not immune to poisoning either, as they can bemanipulated during ne\-tuning. In an attack calledPoisonGPT, researchers recently demonstrated thatsurgical modications to an existing GPT\-based model withthe use of a technique called Rank\-One Model Editing canmake it spread attacker\-controlled disinformation whileperforming just as well as the original model on all the othertopics.between the nodes. The local training dataset is private toAnother use case for data poisoning is code generation andthe participating device and is never shared outside of it.automatic code suggestion tools that help developers writeFederated learning helps companies maximize the amountand diversity of the training data while preserving the dataprivacy of collaborating users. Its not surprising, then, thatthis approach has become widely used in solutions rangingprogramming code. Poisoning the training dataset ofunderlying AI models can force these tools to suggestinsecure, vulnerable, or malicious code. This wasdemonstrated in the TrojanPuzzle attack.from everyday\-use mobile phone applications to self\-drivingText\-to\-image models can also be poisoned to render themcars, manufacturing, and healthcare. However, delegatinguseless. Nightshade is a tool intended for artists who dontthe model training process to an often random andwant their visual art to be used to train AI models but stillunveried cohort of users amplies the risk of training\-timewish to publish their work online. Nightshade allows usersattacks and model hijacking.Data poisoning attacks are relatively easy to perform evenfor uninitiated adversaries because creating pollutedinput can often be done intuitively without specialistknowledge. Such attacks happen daily, from manipulatingtext completion mechanisms to inuencing productreviews to political disinformation campaigns.to add special invisible modications to their images. If acertain amount of Nightshade\-modied images is used inthe training of a generative AI model, the model will ceaseto produce reliable outputs.15PART 2: RISKS FACED BY AI\-BASED SYSTEMSData Poisoning in Academic Research2023 marked a signicant turning point in AI academicresearch, with a heightened focus on potential risks ofpoisoning attacks on large language models (LLMs) anddiffusion models. These advanced models, which extractvast quantities of data from the internet, present achallenge for manual auditing due to their sheer scale andcomplexity. This makes them particularly susceptible totargeted poisoning efforts, where malicious data could beintroduced into the training set to manipulate the modelsbehavior.Researchers have studied various strategies to 
detect and prevent such attacks:Universal Jailbreak Backdoors fromPoisoned Human Feedback \- Rando andTramr discuss a new threat in RLHF\-trainedmodels, where attackers embed a universalbackdoor trigger to provoke harmful respons\-es. They showcase the challenges in creatingrobust defenses against such attacks.Model EvasionAttacks performed against an AI model after it has beendeployed in production, whether on the endpoint or in thecloud, are called inference attacks. In this context, the termText\-to\-Image Diffusion Models can beinference describes a data mining technique that leaksEasily Backdoored through Multimodal Datasensitive information about the model or training dataset.Poisoning \- Zhai et al. explore backdoorattacks on text\-to\-image diffusion models andpropose the BadT2I framework for injectingbackdoors at different semantic levels.Poisoning Web\-Scale Training Datasets isPractical \- Carlini and colleagues introducetwo new dataset poisoning attacks, highlight\-ing the feasibility of purchasing expireddomains linked to various datasets to re\-hostpoisoned data.Poisoning Language Models During Instruc\-tion Tuning \- Wan, Wallace, Shen, and Kleindemonstrate how adversaries can insertpoison examples into user\-submitted data\-sets, manipulating model predictions withtrigger phrases.Knowledge is inferred from the outputs the model producesfor a specially prepared data set. The attackers dont needprivileged access to the model artifacts, training data, ortraining process. The ability to query the model and see itspredictions is all that is needed to perform an inferenceattack. This can be done through the regular UI or APIaccess that many AI\-based systems provide to customers.By repetitively querying the model with specially craftedrequests \- each just a bit different from the previous one \-and recording all the models predictions, attackers cancomprehensively understand the model or the trainingdataset. This information can be used in, for example,model bypass attacks. It can also help reconstruct themodel itself, effectively stealing it.16PART 2: RISKS FACED BY AI\-BASED SYSTEMSAttacks Against AI \- Model EvasionTraining
DataTraining
ProcessTRAININGTrained
ModelInputDecision
ProcessPredictionPRODUCTIONEvasion attacks, also known as model bypasses, aim to security solutions. The earliest application was againstintentionally manipulate model inputs to produceML\-based spam lters designed to predict which emails aremisclassications.Maliciously crafted inputs to a model are referred to asadversarial examples. Their purpose is typically to evadecorrect classication or trigger specic attacker\-denedoutcome. They can also help an attacker learn the decisionboundaries of a model.junk based on the occurrences of specic words in them.Spammers quickly found their way around these lters byadding words associated with legitimate correspondenceto their messages.Similar techniques bypass malwaredetection engines, intrusion detection systems, frauddetection, biometric authentication, and visual recognition.To create an adversarial example, the attacker manipulatesModel Evasion in the Wildthe input in such a way that the model classication of thisinput changes. The difference between the original and themanipulated input often remains imperceptible to humans.For instance, in a visual recognition system, the attackercould modify an image by adding a layer of noise invisible tothe human eye \- or even rotating the image, or changing asingle pixel.This would cause the AI model to give the wrongprediction. Attackers usually send large amounts of slightlydifferent inputs to the model and record the predictionsuntil a sample that triggers the desired misclassication isfound.One of the rst notable instances of an AI evasion attackwas demonstrated in 2019 by Skylight Cyber researcherswho targeted a leading anti\-malware solution. Theresearchers had created a universal bypass against theAI\-based endpoint malware classication model. The attackused inference to determine a subset of strings that, whenembedded in malware, would trick the AI model intoclassifying malicious software as benign. This attackspawned several anti\-virus bypass toolkits such asMalwareGym and MalwareRL, where evasion attacks havebeen combined with reinforcement learning toThis evasion technique can also apply to any other modelautomatically generate mutations in malware that make ittypes used for classication. Its been used in the wild forappear benign to malware classication models.some time, mostly by cybercriminals trying to bypass17PART 2: RISKS FACED BY AI\-BASED SYSTEMSSecurity vendors that provide AI\-based technology (be it2023 saw several papers on attacking LLMs:antivirus, spam lter, IDS, or authenticationauthorizationsystems) have long faced evasion attacks from cybercriminals trying to bypass detection. The same is true fornancial institutions and their fraud preventionmechanisms.These attacks could also be used to hijack self\-driving cars,as they have shown in the past. Researchers demonstratedUniversal and Transferable AdversarialAttacks on Aligned Language Models \- Zouet al. introduce an approach to generateadversarial suffixes that cause aligned LLMsto produce objectionable content.that putting a specially crafted (but innocent\-looking)Paraphrasing evades detectors of AI\-gener\-sticker on a STOP sign can fool on\-board models toated text, but retrieval is an effectivemisclassify the sign and keep driving. Similarly, attackersdefense \- Krishna and colleagues demon\-wanting to bypass a facial recognition system might designstrate that paraphrasing AI\-generated texta special pair of sunglasses that will make the wearercan evade detection algorithms but propose ainvisible to the system. An adversarial state could try toretrieval\-based defense mechanism.evade satellite imagery object detection systems used bythe military to recognize planes, vehicles, and militaryAre aligned neural networks adversariallystructures. The Russian Air Force already used a crudealigned? \- Carlini et al. explore the vulnerabil\-bypass of this sort by painting fake bomber shapes on theity of aligned LLMs to adversarial examplestarmac to fool satellite photo recognition systems intoand the potential for multimodal models to bethinking these are real planes. The possibilities are endless,attacked via image perturbations.and some can have potentially lethal consequencesModel Evasion in Academic ResearchOn the defense side, we saw:Despite continuous advancements in AI and machinelearning, preventing adversarial attacks remains elusive.The same vulnerabilities that compromise the integrity ofimage recognition systems are also found in large languagemodels (LLMs), making them susceptible to similaradversarial manipulations. However, recent research hasshown promising developments in defending imagerecognition models using diffusion models trained on giantdatasets. These advancements suggest a potential pathwayto enhancing the robustness of both image and languagemodels against adversarial threats.Better Diffusion Models Further ImproveAdversarial Training" \- Wang et al. show thatadvanced diffusion models can enhanceadversarial training.Baseline Defenses for Adversarial AttacksAgainst Aligned Language Models \- Jain etal. evaluate various defense strategies againstadversarial attacks on LLMs.18PART 2: RISKS FACED BY AI\-BASED SYSTEMSOn Evaluating Adversarial Robustness ofThe Internal State of an LLM Knows WhenLarge Vision\-Language Models \- Zhao andit's Lying \- Azaria and Mitchell demonstrateteam propose a method to evaluate thethat hidden layer activations of an LLM arerobustness of large VLMs against adversarialdifferent when the model is directed to beattacks.Model Theftevasive or output falsehoods compared towhen the LLM is directed towardstruthfulness.So far, weve focused on scenarios in which adversaries aimthe model (e.g through a GUI or an API). This is enough forto inuence or mislead the AI, but thats not always thethe adversary to perform an attack and attempt to replicatecase. Intellectual property theft stealing the modelthe model or extract sensitive data.itself is a different but credible motivation for an attack.Companies invest time and money to develop and trainadvanced AI solutions that outperform their competitors.Even if information about the model and the dataset itstrained on is not publicly available, users can usually query20% of IT leaders say their 
company are planning and 
testing for model theftjackingAttacks Against AI \- Model TheftTraining
DataTraining
ProcessTrained
ModelInputsPredictionsDecision
ProcessReconstructed
ModelTRAININGPRODUCTION19PART 2: RISKS FACED BY AI\-BASED SYSTEMSIn oracle attacks, adversaries use inference in order toExtraction attacks can result in intellectual property theft,learn details about the model architecture, its parameters,while inversion and membertship inference attacks pose aand the training dataset, and build understanding ofrisk to the privacy of the data the model was trained on.potential points of vulnerability. These attacks can aid theadversary in designing a successful model bypass byModel Theft in the Wildcreating a so\-called surrogate model, a replica of thetargeted model that is then used to assess the modelsdecision boundaries.But these attacks can also have merit on their own. Forexample, the attacker might just be interested inreconstructing the sensitive information the model wastrained on or creating a near\-identical model \- de factostealing the intellectual property. A dirty\-playing competitorcould attempt model theft to give themselves a cheap andeasy advantage without the hassle of nding the rightdataset, labeling feature vectors, and bearing the cost oftraining the model. Stolen models could even be traded onunderground forums in the same manner as condentialsource code and other intellectual property.The NIST Taxonomy and Terminology of AdversarialMachine Learning breaks down oracle attacks into threemain subcategories:Extraction attacks, which attempt to extract thestructure of the model itself based on theobservation of the model's predictionsInversion attacks, which attempt to reconstructthe training data of a model, such as the privatepersonal information of an individualMembership inference attacks, which try todetermine whether a specic sample belongs tothe model's training datasetIn one of the rst demonstrated examples of model theft,researchers created a replica of the ProofPoint emailscoring model by stealing scored datasets and trainingtheir own copycat model. This research was presented atDerbyCon 2019\.In early 2023, Stanford University researchers ne\-tunedMeta's AI LLaMA model and released it under the nameAlpaca, while OpenLM published a permissively licensedopen\-source reproduction of LLaMA called OpenLLaMA.These proved yet again that with sufficient API access, it'spossible to clone even a large and complicated model tocreate a very efficient replica without the hassle of trainingthe model.More recently, OpenAI accused ByteDance \- the companybehind the TikTok platform \- of actively using OpenAIsChatGPT technology to build a rival chatbot. Thesepractices were deemed in violation of OpenAIs terms ofservice, and ByteDances account was promptly suspended.Attempts at stealing technology are already occurring \-even at the highest level, between market\-leadingcompanies.Attacks Specic to Generative AIThe rise of generative AI has spurred new ethical andsecurity challenges. We discussed implications of thepotential misuse of this technology earlier in this report.Lets now look at how adversaries can attack generative AIsystems.20PART 2: RISKS FACED BY AI\-BASED SYSTEMSPrompt InjectionTo prevent their solutions from being maliciously used,most GenAI providers implement extensive securityrestrictions regarding the output available to users. Theserestrictions lter any content deemed harmful or offensive,block access to illegal or dangerous information, andprevent bots from assisting in attack planning, malwaredevelopment, or other illegal activities. They also ensurethat the output doesnt leak sensitive data and compliesIndirect Prompt InjectionIn another recently demonstrated attack, called IndirectPrompt Injection, researchers turned the Bing chatbot intoa scammer to exltrate sensitive data. Bing Chat, by design,can request permissions to access all open tabs and thecontent of the websites on these. An attacker can craft amalicious website containing a specially designed promptthat will modify Bing Chats behavior for as long as thewebsite is open in the victims browser and Bing has accessto the tabs. Adversaries can use this attack to exltratewith applicable policies and laws. Such lters, however, canbe easily bypassed by so\-called prompt injection.specic sensitive information, manipulate users intodownloading malware, or simply mislead and spreadPrompt injection is a technique that can be used to trick anAI bot into performing an unintended or restricted action.This is done by crafting a special prompt that bypasses themodels content lters. Following this special prompt, theOnce AI models begin to interact with APIs at an evenlarger scale, theres little doubt that prompt injectionattacks will become an increasingly consequential attackmisinformation.chatbot will perform an action that was originally restrictedvector.by its developers.There are several ways to achieve this, depending on theCode Injectionmodel type, its exact version, and the tuning it receives.In most cases, GenAI models can only generate the type ofBelow are examples of prompt injection that were able tooutput they are designed to provide (i.e text, image, orbypass ChatGPT restrictions:sound). This means that if somebody prompts anIgnore previous instructions promptactions. However, it might generate a plausible fake outputwhich would suggest these actions were in fact executed.LLM\-based chatbot to, for example, run a shell command orscan a network range, the bot will not perform any of theseDeveloper Mode promptDAN (Do Anything Now) promptAIM (Always Intelligent and Machiavellian) 
promptOpposite mode or AntiGPT promptRoleplaying with the bot, i.e any kind of prompt 
in which the bot is instructed to act as a specic 
character that can disclose restricted data, such 
as the CEO of a company.That said, HiddenLayer discovered (to our utmost disbelief)that certain AI models can actually execute user\-providedcode. For example, Streamlit MathGPT application, whichanswers user\-generated math questions, converts thereceived prompt into Python code, which is then executedby the model in order to return the result of the calculation.Clearly, text generation models are not yet very good atmath themselves, and sometimes need a shortcut. Thisapproach just asks for arbitrary code execution via promptinjection. Needless to say, its always a tremendously badidea to run user\-supplied code.21PART 2: RISKS FACED BY AI\-BASED SYSTEMSSupply Chain AttacksSupply chain attacks occur when a trusted third\-partyvendor is the victim of an attack and, as a result, theproduct you source from them is compromised with amalicious component. Supply chain attacks can beincredibly damaging, far\-reaching, and an all\-aroundterrifying prospect that has been carved into the collectivememory of the security community through major attackssuch as SolarWinds and Kaseya among others.In those attacks hundreds, if not thousands, ofTwo key factors make supply chain attacks so successfuland dangerous: the exploitation of trust and the reach ofthe attack.Trust the attacker abuses the existing trustbetween the producer and consumer. Given thesuppliers prevalence and reputation, theirproducts often garner less scrutiny and canreceive more lax security controls.Reach the adversary can affect thedownstream customers of the victimorganizations in both the public and private sectors wereorganization in one fell swoop, achieving aaffected. They resulted in a range of security breaches and,one\-to\-many business model.in some cases, ransomware. These incidents serve as astark reminder of why we do cybersecurity in the rst place,and a warning not to repeat the same mistakes. Yet, theThe ML supply chain is a vast ecosystem of different tools,ground underneath has shifted once again, requiringlibraries, and services developed by household names andorganizations to adapt security controls to the age of AI.industry newcomers alike. From ML frameworks to Machine75%of IT leaders say that 
third\-party AI integrations are 
riskier than existing threatsLearning Operations (MLOps) tooling and modelrepositories, each plays a fundamental role indemocratizing AI and accelerating the pace of progresswithin the eld. However, with so many moving parts andnew technologies to wrestle with, they inadvertentlyintroduce new supply chain risk, leaving us vulnerable torepeating the mistakes of the past.ML Supply Chain AttacksDATA
COLLECTIONMODEL
SOURCINGML OPS
TOOLINGBUILD \&
DEPLOYMENT Data poisoning Hijacked Models
 Backdoored Models Software Vulnerabilities
 Compromised Packages Prediction Tampering
 Build Environment
CompromiseVULNERABILITIES OF THE ML SUPPLY CHAIN22PART 2: RISKS FACED BY AI\-BASED SYSTEMSThe parts of the machine learning supply chain thatHiddenLayer identied as posing the most signicantMalicious Modelsrisk are:Malicious modelsModel backdoorsSecurity of public model repositoriesWhen a machine learning model is stored to disk, it has tobe serialized, i.e translated into abinary form and saved asa le. There are many serialization formats and each of theML frameworks has its own default ones. Unfortunately,many of the most widely used formats are inherentlyvulnerable to arbitrary code execution. These includePythons Pickle format (used by PyTorch, among others),HDF5 (used for example by the Keras framework), andMalevolent 3rd\-party contractorsSavedModel (used by TensorFlow).Vulnerabilities in ML toolingData poisoningVulnerabilities in these serialization formats allowadversaries to not only create malicious models, but alsohijack legitimate models in order to execute maliciouspayloads. Such hijacked models can then serve as an initialaccess point for the attackers, or help propagate malwareto downstream customers in supply chain attacks.Exploiting ML Serialization \- Code ExecutionTorchScriptHDF5SavedModelOver the last year, HiddenLayer identied numerousthat will trigger when the model is loaded. These attackshijacked models in the wild which contained maliciousare proving fruitful in bug bounty programs, as was shownfunctionality, such as reverse shells and post\-exploitationat DEF CON 31 AI Village. There, Threlfall Hax spoke aboutpayloads. As a potential worst case scenario, we alsohow he had compromised several organizations as part ofdemonstrated how machine learning models could betheir bug bounty program using malicious models deployedabused to hide and deploy ransomware payloadson Hugging Face that went undetected on the platform.23PART 2: RISKS FACED BY AI\-BASED SYSTEMSExpand ITWModel BackdoorsBesides injecting traditional malware, a skilled adversarya pre\-trained model and introduce a secret unwantedcould also tamper with the model's algorithm in order tobehavior to the targeted AI. This behavior can then bemodify the model's predictions. It was demonstrated that atriggered by specic inputs, as dened by the attacker, tospecially crafted neural payload could be injected intoget the model to produce a desired output. Its commonlyreferred to as a model backdoor.AI Algorithm BackdooringTriggerCond.
moduleManipulated
ImageBenign ImageTURTLEPREDICTIONCAT24PART 2: RISKS FACED BY AI\-BASED SYSTEMSA skillfully backdoored model can appear very accurate onlike Hugging Face, offer a range of free pre\-trained models.the surface, performing as expected with the regularHugging Face alone consists of over 500,000\.dataset. However, it will misbehave with every input that ismanipulated in a certain way a way that is only known toThese models are trivial to download and install in your ownthe adversary. This knowledge can then be sold to anyapplication, especially with libraries like Transformersinterested party or used to provide a service that will ensureenabling developers of all skill levels to utilize machinecustomers always get a favorable outcome (for example inlearning. If an attacker nds a way to breach the repositoryloan approvals, insurance policies, etc.)the model is served from, they could then replace it with ahijacked or backdoored version and cause majorSecurity of Public Model Repositoriesdownstream consequences.Many ML\-based solutions are designed to run locally andare distributed together with the model. We dont have tolook further than the mobile applications hosted on GooglePlay or Apple Store. Moreover, specialized repositories, ormodel zoos,85%of companies are using 
pre\-trained models from 
public repositories to 
jumpstart innovation.Supply Chain AttacksPYTHON
LOADERML MODELPickle InjectionUploadDeploymentSteganographyPAYLOADLateral
MovementRANSOMWAREBACKDOORSSPYWARECOIN MINERSMalevolent Third\-party ContractorsMaintaining the competitiveness of an AI solution in aSuch an approach can save time and money, but it requiresrapidly evolving market often requires solid technicaltrust, as a malevolent contractor could plant a backdoor inexpertise and signicant computational resources. Smallerthe model they were tasked to train. If your model is beingbusinesses that refrain from using publicly available modelsused in a business\-critical situation, you may want to verifymight instead be tempted to outsource the task of trainingthat those youre sourcing the model from know whattheir models to a specialized third party.theyre doing, and that theyre of sound reputation.25PART 2: RISKS FACED BY AI\-BASED SYSTEMSVulnerabilities in ML ToolingIts easy to see that with incredibly large data sets, it can bedifficult to police with a high level of delity.A wide variety of tooling is used throughout the industry tosupport the development, deployment, and testing ofmachine learning models. There are a huge number oflibraries and frameworks that make up parts of thisecosystem, each with their own use\-cases, advantages, anddisadvantages. However, many of these tools lackadequate security controls \- and in some cases dont evenhave basic authentication. With the vast amounts of oftensensitive information that these models consume, this canbe an especially worrying concern for a data breach.Ultimately, this is a result of security having been anafterthought in the development of ML tooling. Highseverity vulnerabilities are regularly reported in popularML\-centric and ML\-adjacent libraries, such as MLOpsframeworks. These vulnerabilities can be exploited tocompromise build environments and leak volumes ofsensitive training data, or worse \- proliferate a damagingsupply chain attack similar to that of the SolarWindsbreach.The last year has seen attacks such as a malicious PyTorchnightly build which was compromised via the torchtritonpackage, allowing the attacker to exltrate data fromaffected hosts.Data Poisoning in Supply Chain AttacksThe quality of a model greatly depends on the quality of itsdata. The story that the data tells will be reected by themodel. For example, if theres a bias in the data, there will bea bias in the model's output. For this reason, its incrediblyimportant to understand where youre sourcing your datafrom, and if your data is what you think it is. This is both forefficacy purposes, and to make sure that an attacker hasntpoisoned your data by introducing bias, reducing modelaccuracy, or planting a backdoor.Recently, research from Carlini et al demonstrated how theycould poison web\-scale datasets which consisted of links tothe data, instead of the data itself. By buying up expireddomains that were listed within the dataset, and hostingtheir own malicious data in its stead, they were able topoison any models created from this dataset. Whats morethey were able to poison 0\.01% of the LAION\-400M orCOYO\-700M datasets, for as little as $60\. The researchersalso discussed the possibility of poisoning up to 6\.5% ofWikipedia by exploiting rolling snapshots on the site andtiming their edits of pages accordingly.Whats concerning about these types of attacks is that you,or the creators of the model youre using, may be blissfullyunaware that the data was poisoned to begin with, leadingto potentially catastrophic downstream incidents.To learn more about supply chain attacks within the contextof AI applications, check out the blog Insane in the SupplyChain.Threat Actors and Attack VectorsAttacks on AI systems are already taking place in the wild,but the real scale to which they happen is difficult toassess. This attack vector is still very new, meaning thatthere is not enough awareness about it. As a result,security solutions that could detect such attacks are fewand far between.Model hijacking attacks, in which AI models are used todeliver traditional malicious payloads, are the easiest onesto spot. This is because existing software security conceptscan be extended to detect and prevent such attacks.26PART 2: RISKS FACED BY AI\-BASED SYSTEMSThey are also, from the attacker's perspective, the easiestones to perform. The widespread lack of digital signing,integrity checking, and anti\-virus scanning of AI artifactsmakes them an enticing target for traditional cybercrime.Many security researchers have been subverting MLmodels to achieve code execution for proof\-of\-conceptpurposes. But it's not just security researchers that arelooking into this attack vector.Several instances ofhijacked models can likely be attributed to malicious actors.This includes models containing reverse\-shells, as well asCobaltStrike and Metasploit stagers, all of which wereconnected to known malicious command and controlcenters.Because hijacked models are often uploaded to publicrepositories, there is some visibility into them. However, thesituation gets much more complicated with data poisoning,model evasion, and model theft attacks. Most businessesdo not monitor their AI for adversarial inputs.Those who do are not obliged to disclose that they'venoticed malicious activity. Therefore, the details ofadversarial attacks are rarely made public. Whatever isdisclosed is most likely just a tiny tip of an iceberg \- and theiceberg is poised to grow exponentially over the comingyears, as more and more adversaries target AI systems.The scarcity of information means it is too early to have asolid insight on threat intelligence regarding attacks on AIsystems. However, it's denitely a good time to initiatediscussion around it, and start collecting and organizingdata.27PART 3:ADVANCEMENTS IN
SECURITY FOR AIBefore anyone can start implementing protections forcertain technologies, the industry needs to gure outthe ways in which these technologies are vulnerable.This is why offensive security plays such a big role inplanning the defenses.When a new technology comes out, white\-hat researchersto NIST AI Risk Management Framework, to varioustry to get one step ahead of the attackers and come upnational and international policies and regulations,with proof\-of\-concept scenarios for potential attacksdefensive measures are now being implemented to lay theagainst this technology. Defensive solutions are oftengroundwork for securing AI.built upon previous offensive research and attack tooling.Security for AI is no different. The rst research papers andtools in this eld were also of the offensive kind. For quiteOffensive Security Tooling for AIsome time, attacks against AI were mostly covered inOffensive security tooling has been around for a long time,academia papers, with exercises performed by securityenabling red teams and pen testers to evaluate IT systemsprofessionals. However, the last couple of years havefor possible weaknesses. Although initially designed withmarked a massive shift.security in mind, these frameworks have provenincreasingly useful to malicious actors, enabling them toWith AI\-based systems being rapidly implemented acrossperform attacks with ease while only requiring an abstractsectors, there has also been a substantial rise inunderstanding of how the attack works under the hood.intentionally harmful attacks. The need for defensivesolutions is now front and center. From MITRE ATLASknowledge base,28PART 3: ADVANCEMENTS IN SECURITY FOR AIProjects such as Metasploit, Cobalt Strike, and Empire areThey offer various attack techniques from bypass to theft tonow as much associated with malicious activity as they arecode execution. Although very valuable in improving thewith red\-teaming. The concept of offensive security hassecurity, safety, and robustness of the models, they can alsoalso made its way to the eld of articial intelligence, wherebe used by adversaries in malicious activities and makeAI security researchers have developed various tools to testattacking AI more straightforward and accessible than ittheir attack techniques.might at rst seem.There are many publicly available security evaluation toolsdesigned to test AI systems.Automated Attack FrameworksAdversarial ML FrameworksOne of the rst libraries for testing the robustness of AI2020 saw the release of Armory, a containerized testingsystems against adversarial examples, called CleverHans,tool for evaluating adversarial defenses, which interfacesdates as far back as 2016\. In 2018, IBM released itswith IBMs ART. In 2021 Facebook released AugLy, a dataAdversarial Robustness Toolbox (ART), a framework thataugmentation library, which can be potentially used toimplements a multitude of attacks against AI and includesgenerate adversarial examples. Microsoft followed witheasy\-to\-follow Jupyter Notebook examples. MLSploit, athe release of Countert, an easy\-to\-use command\-lineuser\-friendly cloud\-based framework whose name calls outautomation layer for security evaluation of ML models,to Metasploit, was released in 2019; it allows for thewhich interfaces with existing attack tools andcreation of attacks on various malware classiers, intrusionframeworks, including ART, TextAttack, and AugLy, anddetectors, and object detectors. In the same year, QDataresembles Metasploit in terms of commands andreleased TextAttack, a powerful model\-agnostic NLP attacknavigation.framework that can help perform adversarial text attacks,text augmentation, and model training.29It was developed to demonstrate the vulnerability of MLmodels against more traditional attacks, but can be used byscript kiddies to subvert publicly available models.Defensive Frameworks for AIWith new tools and techniques for attacking AI poppingup with increasing frequency, it has become clear that amethodical defensive approach is needed to safeguardthis booming technology.Over the last two years, several big cybersecurity playershave created comprehensive frameworks comprisingvarious security practices, strategies, andrecommendations for AI. These frameworks are incrediblyvaluable rst steps on the road long ahead.Defensive FrameworksPART 3: ADVANCEMENTS IN SECURITY FOR AIAnti\-Malware Evasion ToolingIn addition to robustness evaluation frameworks, thereare also more specialized tools that aim at a specicoutcome. MalwareGym, for example, helps bypassAI\-based anti\-malware solutions. Released in 2017anti\-virus company Endgame, it implementsreinforcement learning in the modication of Windowsapplications. By taking features from benign executablesand adding them to malicious ones, MalwareGym cancreate malware that bypasses malware scanners.Although MalwareGym is just a demo tool against onespecic classier, it has a successor in the MalwareRLproject, which was released in 2021 and supports attacksagainst three different classiers.Model Theft ToolingAnother type of adversarial tool is KnockOffNets, releasedby researchers at the Max Planck Institute for Informaticsin 2021\. Its a tool for creating a replica of an AI model or, inother words, for stealing the model. It requires no previousknowledge of the model or the training data. The authorsclaim it can relatively accurately reproduce a model for$30\. Although KnockOffNets was created to showcase theease of model theftmodel extraction attacks, it can alsohelp adversaries build their own model theft tooling.Model Deserialization ExploitationFickling, released in 2021 by Trail of Bits, is the rst tool toexploit one of the most popular AI model serializationformats: Pythons pickle format. It contains a decompiler,static analyzer, and bytecode rewriter and can injectarbitrary code into AI models saved as pickles. Charcuterie,released in 2022, implements a set of attacks that utilizecode execution techniques and deserialization exploits inML models.30PART 3: ADVANCEMENTS IN SECURITY FOR AIMITRE ATLASFirst released in 2020 on GitHub then launched as a fullwebsite in 2021, MITRE ATLAS stands for Adversarial ThreatLandscape for Articial\-Intelligence Systems. ATLAS is aknowledge base of adversarial machine learning tactics,techniques, and case studies designed to helpcybersecurity professionals, data scientists, and theircompanies stay up to date on the latest attacks anddefenses against adversarial machine learning. The ATLASmatrix is modeled after and complementary to the MITREThis survey demonstrates the prominence ofreal\-world threats on AI\-enabled systems, with77% of participating companies reportingbreaches to their AI applications this year. TheMITRE ATLAS community is dedicated tocharacterizing and mitigating these threats ina global alliance. We applaud our communitycollaborators who enhance our collectiveability to anticipate, prevent, and mitigaterisks to AI systems, including HiddenLayer andATT\&CK framework, which is well\-known and used in thetheir latest threat report.cybersecurity industry to understand attack chains and Dr. Christina Liaghati, MITRE ATLAS Leadadversary behaviors.The ATLAS matrix is broken down into two maincomponents: Tactics and Techniques. The tactics describeMITRE ATLAS Updateswhat an adversary is trying to accomplish. For example,Reconnaissance to learn more about a model deploymentor Exltration to steal the model itself. The techniques, onthe other hand, describe how an adversary is going toaccomplish their tactic. Taking the Reconnaissanceexample, an attacker may Search Victim\-Owned Websitesfor information about models or those internally whocontrol or interact with them.One difference between ATLAS and its ATT\&CK counterpartis the source of the techniques. While ATT\&CK is based onlyon detected attacks in the wild, ATLAS uses unique casestudies selected from their impact to productionAI\-enabled systems. These case studies are a combinationof both real\-world attacks discovered in the wild as well asrealistic red\-teaming exercises from AI red teams orsecurity groups. Some of these white hat hacker attacks arecompletely undetectable but have valuably demonstrated arealistic attack pathway that could threaten real\-worldAI\-enabled systems. Including case studies of bothmalicious attacks and the white hat hacker attacks inATLAS provides a more grounded and complete picture ofthe AI\-enabledsystem threat landscape. These casestudies outline who attack victims are and map to varioustechniques observed within the full scope of the attack.In 2023, the ATLAS team released several major updatesand new tools to continue enabling organizations that areworking to secure their AI\-enabled systems. These releasesincluded:A signicant update to the matrix to ground therapidly evolving attack pathways for LLMs andGenAI enabled systems. This update added 12 newtechniques and 5 unique case studies to ATLAS asthe result of a close collaboration with Microsoftand other ATLAS community members determinedto realistically represent these new LLM attackpathways.Arsenal and Almanac plugins developedcollaboratively with Microsoft to addimplementations of ATLAS techniques and newadversary proles that target AI\-enabled systemsto CALDERA, an existing MITRE open\-sourcethreat emulation tool largely leveraged by thecyber world.31PART 3: ADVANCEMENTS IN SECURITY FOR AIAn initial release of 20 new mitigations based onATLAS case studies that provide high\-levelinformation on the security concepts and classesof technologies that can be used to prevent anadversarial attack technique from beingsuccessfully executed. The framework splits itself into two parts: framing the risksrelated to AI systems and the core framework itself. Thecore describes four functions: govern, map, measure, andmanage. Each breaks down into further controls to giveorganizations greater insights securing their AIinfrastructure.To date, ATLAS now has 14 Tactics, 82 Techniques, 22 CaseStudies, and 20 Mitigations. As the ATLAS team continuesto work with leading AI security organizations and expertsacross government and industry to expand the frameworkand its related tools and capabilities, the community\-drivenknowledge base and tools will remain a critical groundingresource as we all work to better secure our AI\-enabledsystems and supply chain against attacks.In 2024, the MITRE ATLAS team will continue building uponthe existing framework, tools and capabilities to help thecommunity navigate the landscape of threats to AI\-enabledsystems by expanding on their platforms for both publicGoogle Secure AI FrameworkIntroduced by Google in June 2023, Secure AI Framework(SAIF) is a conceptual framework that, like NIST AI RMF,provides guidance on securing AI systems.It builds uponbest practices and experience from traditional softwaredevelopment, adapting them to t the needs of AI systems.There are six core elements to the framework:Expand strong security foundations to the 
AI ecosystemvulnerability reporting and protected incident sharing.Extend detection and response to AIThrough continued collaborations with industry, academia,and government, the ATLAS team is evolving open\-sourceresources like the AI Risk Database, a tool for discoveringvulnerabilities associated with public AI models. While thepublic ATLAS website continues to publicly representunique real\-world attacks, the ATLAS team is alsocontinuing to expand its platform for more rapid protectedor anonymized threat sharing within its community.NIST AI Risk Management FrameworkIn January 2023, US National Institute of Standards andTechnology released the AI Risk Management Framework(AI RMF). The AI RMF is a conceptual framework that takeslearnings from the traditional software and information\-based systems and applies them to the unique challengespresented by AI systems. It provides guidance forresponsible design, development, deployment, and use ofAI systems to give organizations additional trust in AI.Automate defenses to keep pace with 
existing and new threatsHarmonize platform level controlsAdapt controls and mitigations for AI 
deploymentContextualize AI risks to match business 
processes.As with many other frameworks, SAIF will review traditionalsecurity controls around data and network level access,AIML specic controls such as data poisoning anddetecting anomalies, privacy requirements and regulations,as well as governance around the entire lifecycle.32PART 3: ADVANCEMENTS IN SECURITY FOR AIOWASP Top 10The Open Worldwide Application Security Project (OWASP)is a non\-prot organization and online community thatprovides free guidance and resources, such as articles,documentation and tools in the eld of application security.The OWASP Top 10 lists comprise the most critical securityrisks faced by various web technologies, such as accesscontrol and cryptographic failures.In 2023, OWASP released the Top 10 Machine Learning risks.These controls help those who are building, operating, andsecuring machine learning to identify potential risks andattack vectors within their deployments. Each of theindividual controls has information on the attack vector,various risk factors that can help with threat modeling, andguidance on how to prevent the attack. When combinedwith other practical guidance from other frameworks suchas ATLAS, this helps demystify the real threats to machinelearning and what can be done about them.Another recent release from OWASP are the top 10 criticalvulnerabilities seen in Large Language Models (LLMs). Withthe rapid recent adoption of LLM technology, risksDatabricks AI Security Framework 
(DAISF)The DAISF framework adopts a comprehensive strategy tomitigate cyber risks in AI systems. It provides insights intohow ML impacts system security and how to apply securityengineering principles to AI systems. It also offers a detailedguide for understanding the security and compliance ofspecic ML systems.Actionable defense recommendations apply to 
12 foundational components of a generic 
data\-centric AI system:raw datadata preparationdatasetsdata and AI governancemachine learning algorithmsevaluationmachine learning modelsassociated with deploying LLMs have been proliferating (asmodel managementdiscussed in section 2\). This OWASP document coversitems such as prompt injection, output handling, all the wayto model theft of the LLM itself. Each section also offerspractical guidance for using this technology in aresponsible and secure manner.Gartner AI Trust, Risk, and Security 
Management (AI TRiSM)Gartner dened a framework to address concerns aroundAI and ML systems, called AI TRiSM. It covers challengessuch as bias, privacy, and explainability while also touchingon the security and risks of such systems. This provides aroadmap for organizations to build AIML systems thatmaintain trust, are reliable and fair, and secure by design.model serving and inferenceinference responsemachine learning operationsdata and AI platform securityWithin these components, Databricks identied 54technical security risks. Their recommendations are basedon the real\-world evidence that adversaries compromiseunsecured AI systems using simple tactics.33PART 3: ADVANCEMENTS IN SECURITY FOR AIIBM Framework for Securing 
Generative AIIn January 2024, IBM released their Framework for SecuringGenerative AI, focused on the use of LLMs and other GenAIsolutions in businesses and organizations. It providesdefensive approaches by helping to estimate the most likelyattack that can occur at each stage of the pipeline, andsuggesting relevant safeguards and defenses.Red Teaming and Risk AssessmentFirst ideas of AI red teaming emerged in the late 2010s. Atthat point, AI systems were already known for theirvulnerability to things like bias, adversarial examples, andgeneral abuse. Even though, now, theres widespreadacceptance that AI will dene this decade, it's still mostlythe major players \- such as Google, NVIDIA, or Microsoft \-who invest in building their own internally\-focused teamsdedicated to pentesting the AI solutions they develop andIBMs framework consists of ve steps:implement.Securing the data: describes risks related 
to the data collection and processing 
phase, such as mishandling PII and privacy 
concernsSecuring the model: deals with attacks 
that can occur during model development 
and training, including supply chain 
attacks, API attacks, and LLM exploitation;Securing the usage: relates to the live use 
of model in production and covers 
inference attacks, including prompt 
injection and model theftSecuring the infrastructure: tapping into 
existing expertise to optimize and harden 
network security, access control, data 
encryption, and intrusion detection and 
preventionEstablishing governance: putting 
guardrails in place that ensures AI systems 
dont stray from what they are intended to 
do and act as expected.It would be unfair to mention these companies 
by name and not highlight some of the 
incredible work they have done to bring light to 
the security of AI systems:In December 2021, Microsoft published 
their Best practices for AI security risk 
managementIn June 2023, NVIDIA introduced their red 
team to the world alongside the 
framework they use as the foundation for 
their assessmentsIn July 2023, Google announced their own 
AI red team following the release of their 
Secure AI Framework (SAIF).14%of IT leaders say their company are 
planning and testing for adversarial 
attacks on AI models34PART 3: ADVANCEMENTS IN SECURITY FOR AIPolicies and RegulationsWe've already discussed how AI is a double\-edged sword: itcan be easily turned against people, businesses, andsocieties, with far\-reaching consequences that could provedevastating. For this reason, it is imperative forgovernments around the world to introduce tightregulations on how AI can be used safely, legally, andethically.The rst regulations around the use of AI wereimplemented as part of the European Union's General DataProtection Regulation (GDPR). These were very limited inscope and related mainly to the need for certain AI systemsto be explainable. An AI model is explainable only if itspossible for us, humans, to assess why the model returneda specic prediction. This is important in all applicationsthat make critical decisions, or decisions that can have animpact on people.In 2019, the Organization for Economic Co\-operation andDevelopment (OECD) adopted the Recommendation onArticial Intelligence (the OECD AI Principles). It describesve principles and ve recommendations for OECDcountries and adhering partner economies to promoteresponsible and trustworthy AI policies.In 2022, the EU proposed a more comprehensive AI Actthat groups AI solutions into three categories: low\-riskOn a national level, several countries have startedintroducing AI\-specic legislations. Singapore's Model AIGovernance Framework, whose rst edition dates back to2019, consists of 11 AI ethics principles, includingtransparency, explainability, safety, security, datagovernance, and accountability. Canadas Digital CharterImplementation Act (Bill C\-27\), dated June 2022,encompasses the Articial Intelligence and Data Act (AIDA), which addresses the responsible adoption of AI. The UK iscurrently eshing out its Articial Intelligence (Regulation)Bill, whose purpose is to make provisions for the regulationof articial intelligence.In October 2022, the US introduced the Blueprint for an AIBill of Rights, a set of suggestions and guidelinesconcerning the development and use of AI systems. A yearafter, in October 2023, the US White House issued anexecutive order on the safe, secure, and trustworthydevelopment and use of articial intelligence. The ordersets standards for AI safety and security. It outlines risks AIsystems pose, such as threat to human safety, detection ofAI generated content, as well as securing the AI ecosystem.There are also orders on protecting privacy for citizens fromdata collection and storage to eliminate bias anddiscrimination in machine learning models. Finally, theorder also aims to promote innovation and competition forthose looking to advance and secure AI systems. Theexecutive order mentions the NIST AI RMF multiple times,which will likely be a framework organizations can leverageto guide the secure development and deployment of AIapplications that have to adhere to transparency laws butsystems.are otherwise unregulated; high\-risk applications that aresubject to strict limitations; and applications that aredeemed dangerous and are outright banned. A provisionalagreement between the EU Council presidency and theEuropean Parliament was reached on this proposal inDecember 2023\. It's expected to become law soon.35PART 4:PREDICTIONS ANDRECOMMENDATIONSIts always fun to dust off the crystal ball and try topredict future trends in cybersecurity. AI has been thedominant factor in many threat reports fromtraditional cybersecurity vendors this year. Whilemost focus on generative AI, we take a broader lookat the AI ecosystem and predict how it may be abusedby cybercriminals, nation\-states, and general badactors over the coming year.Predictions for the next 12 months1\. Data scientists will partner with security practitioners to secure their modelsThe cybersecurity industry has been in a technological arms race with adversaries for several decades, as each newadvancement brings unique security concerns that require bespoke security solutions. However, AIML security has beenoverlooked in the data science world; rapid advances in AI and ML often lack even basic security controls. This has led tomany vulnerabilities in libraries and tooling that have become pillars of AI software development. We expect this trend toreverse slightly over the coming year, as researchers work rapidly to uncover vulnerabilities and help shore up defenses inthe open\-source ML projects. The emerging collaboration between data scientists and cybersecurity specialists will boostthe security of the whole AI ecosystem.2\. Supply chain attacks using ML artifacts will become much more commonDue to inherent insecurities in the machine learning tool chain, there are many low hanging fruits for cybercriminals toexploit. Threat actors are increasingly turning their sights towards MLOps platforms and tooling. Look for supply chainattacks to become more common as the year progresses, and not just for traditional initial compromise and lateralmovement purposes. The often sensitive nature of ML models and the data they touch makes them very attractive tocybercriminals. Attackers will increasingly leverage vulnerabilities in MLOps platforms to poison training sets and exltratesensitive data used at train or inference time to gain a competitive advantage or abuse AI systems.36PART 4: PREDICTIONS AND RECOMMENDATIONS3\. There will be a signicant increase in adversarial attacks against AIInversion attacks to infer training data or model details, inference attacks to generate bypassesmisclassications, and,ultimately, model theft attacks will also become much more common. All these attack techniques will be driven byever\-expanding research into adversarial ML by academia and industry which is being made available through easy\-to\-useopen\-source software. What was once a complex undertaking is and will continue to become increasingly simple formere script kiddies to implement.4\. Threat actors will automate hacking efforts with LLMsGenerative AI is where we expect to nd the most signicant can of worms. Cybercriminals already use LLMs to enhanceexisting attacks, from authoring more realistic phishing emails to generating unique malware payloads on the y andimproving social engineering efforts. Its not a stretch to envisage threat actors harnessing LLMs to automate hackingefforts, perform reconnaissance, and supplement cybercrime\-as\-a\-service over the coming year.In addition, as LLMs evolve from text generation to multimodal systems capable of producing text, images, and audio, weexpect a sharp increase in political activists and those trying to inuence society using disinformation.Another interesting development in the world of LLMs is RAG, Retrieval\-Augmented Generation, which enhances the modelwith external sources of information or ground truths. RAG\-empowered LLMs will be ripe for abuse by attackers, who willseek to leak sensitive information using carefully crafted prompts, especially if trained on corporate data.5\. Deepfakes will be increasingly used in scam and disinformationArmed with powerful tools that can generate almost impeccable video and audio, adversaries are poised to become muchmore successful in their attempts at deceiving people, be it for the purpose of defrauding money, extracting sensitiveinformation, or spreading fake news. The traditional scam scenario \- in which the attacker sends a message pretending to bea relative who lost their phone and needs money \- is now acquiring a whole new dimension. Instead of text\-based messages,cybercriminals will be shifting to deepfake audio and video calls, and these can prove challenging not to fall for.The bigger the digital footprint a person leaves behind, the more realistic a deepfake instance of this person can be.Naturally, public gures such as artists, inuencers and politicians will be both the most enticing and vulnerable targets.However, it can take as little as just a few photos to create a deepfake convincing enough to trick a non savvy person intogiving away money or information, and cybercriminals will look to cash in on low prole targets as well.As the political climate deteriorates and tensions grow between nations, state\-sponsored adversaries will use carefullycurated deepfakes to steer public opinion, manipulate political campaigns and disturb elections. Conspiracy theories willhave a wider reach and fake news will become increasingly difficult to disprove. Even if we nd a way to reliably tell authenticvideos from fakes it might not help limiting the damage. Once manipulated, people often refuse to acknowledge facts oraccept the truth. The best solution to prevent deepfake\-induced harm is to prevent the proliferation of deepfakesthemselves.37PART 4: PREDICTIONS AND RECOMMENDATIONS6\. AI attack surfaces will expand while more organizations 
use advanced tools to combat threatsIt has never been easier to develop, use, and implement AI within organizations This rapid integration into establishedprocesses is introducing an ever\-expanding novel attack surface that is not protected by conventional security controls.Businesses will experience many growing pains this coming year, where AI is exposed or congured insecurely, leading todata breaches, compromise, or worse.On the ipside, we also expect to see more widespread adoption of AI security principles across organizations, anddemocratization of advanced methods of monitoring model behavior and model security evaluation, which have beentypically reserved for major enterprises. As a result, many more organizations will be able to identify and take actions againstadversarial attacks.Securing Your AI: Getting Started1\. Discovery and Asset ManagementUnderstanding and implementing extensive securityyour organization. What applications has yourmeasures for AI is no longer a choice. Its a necessity. Tooorganization already purchased that use AI or havemuch is at risk for organizations, government, and societyAI\-enabled features?Begin by identifying where AI is already used inat large. Security must maintain pace with AI to allowinnovation to ourish. That is why it is imperative tosafeguard your most valuable assets, from development tooperation and everything in between.But how should you get started?Let this guide be a starting point to securing your AIsystems. Whether youre a developer, data scientist, or an ITprofessional, ensuring your AI systems are secure willempower you and your organization to navigate the futurecondently.93% of IT leaders say they haveimplemented security for 
AI protocols, but58% arent sure these protocolsare keeping pace with 
evolving threats.Evaluate what AI may be under development byyour organization. How many data scientists ordata engineers roles are you employing? Howmany are you hiring? How many are consultants?Understand what pretrained models from publicrepositories may already be in use. Do you knowwhat websites offer pre\-trained models? Do youunderstand the networkweb traffic to these sitesand who may have already downloaded thesemodels?2\.Risk Assessment and Threat ModelingConduct a benet assessment to identify thepotential negative consequences associated withthe AI systems if those models were to becompromised in any way.Perform threat modeling to understand thepotential vulnerabilities and attack vectors thatcould be exploited by malicious actors to completeyour understanding of your organizations AI riskexposure38PART 4: PREDICTIONS AND RECOMMENDATIONS3\.Data Security and PrivacyGo beyond the typical implementation ofencryption, access controls, and secure datastorage practices to protect your AI model data.Identify the AI security architecture required to beinstrumented for the runtime protection of your AIwhen the models go into production use.6\.Continuous Monitoring and Incident ResponseThose controls will not effectively protect the dataImplement continuous monitoring mechanisms toin your models from theft, alteration, or otherdetect anomalies and potential security incidentsforms of attack. Evaluate and implement securityin real\-time for your AI. Require your vendors tosolutions that are purpose\-built to provide runtimeutilize AI in their solutions to alert you to attacksprotection for AI models. Look for solutions thatthat could compromise your data or businesscan span the vast array of le types, model types,processes.and also be agnostic to on\-prem or clouddeployments.Develop a robust AI incident response plan toquickly and effectively address security breachesEmbed into your 3rd\-party risk process anor anomalies. Regularly test and update theevaluation of your vendors' security for their AIincident response plan to adapt to evolving AIcapabilities. Ask how your vendors incorporatethreats.security into their AI development lifecycle,including how they scan their models for datapoisoning and malicious executables. Find out howthey provide real\-timerun time protection todetect and stop various forms of attacks againstthe AI capabilities embedded in the solutions youbought from them.4\.Model Robustness and ValidationRemember that the security landscape as well as AItechnology are dynamic and rapidly changing. It's crucialto stay informed about emerging threats and best practices.Regularly update and rene your AI\-specic securityprogram to address new challenges and vulnerabilities.And a note of caution. Responsible and ethical AIframeworks in many cases fall short of ensuring modelsare secure before they go into production, as well as afterRegularly assess the robustness of AI modelsan AI system is in use. They focus on things such as biases,against adversarial attacks. This involvesappropriate use, and privacy.While these are alsopen\-testing the model's response to variousrequired, dont confuse these practices for security.attacks such as intentionally manipulated inputs.Implement model validation techniques to ensurethe AI system behaves predictably and reliably inreal\-world scenarios. This will help minimize therisk of unintended consequences.5\.Secure Development PracticesIncorporate security into your AI developmentlifecycle. Train your data scientists, data engineers,and developers on the various attack vectorsassociated with AI. Make sure to include how tominimize potential attack surface early in thesecurity development lifecycle.The nal recommendation: Always ask yourself 
the following questions:What am I doing to secure my 
organization's use of AI?Is it enough?How do I know?Only by answering these questions with data\-driven,intellectual honesty, can you maintain the integrity of yoursecurity role and the critical function it provides.39RESOURCESResourcesHiddenLayer Products and ServicesHiddenLayer AISec Platformis a comprehensive AI security solution that ensures the integrity and safetyof your models throughout the MLOps pipeline. By evaluating the security ofpretrained models, detecting malicious injections, and monitoring algorithminputs and outputs for potential abuse, the AISec Platform delivers anautomated and scalable defense tailored for AI.Learn MoreHiddenLayer Machine Learning Detection \& Response (MLDR)complements your existing security stack, enabling you to automate and scale theprotection of AI models and ensure their security in real\-time. With MLDRintegrated into your MLOps lifecycle and SIEM tools, you can proactively defendagainst threats to AI.Learn MoreHiddenLayer Model Scannerenables you to evaluate security and integrity of your ML artifacts beforedeploying them. This mitigates the risk of supply chain attacks through hijackedor backdoored models. With the Model Scanner, you can identify and remediatepotential risks ensuring a safe and trusted environment.Learn MoreHiddenLayer Professional Servicesleverage deep domain expertise in cybersecurity and apply it to the eld of AI.Our Adversarial Machine Learning Research (AMLR) team is equipped with aunique skill set that encompasses machine learning, reverse engineering, digitalforensics and threat intelligence. We tailor our efforts to empower your datascience and cybersecurity teams with the knowledge, insight, and tools neededto protect and maximize your AI investments.Learn More40RESOURCESGet to Know HiddenLayerAN INTRODUCTION TO HIDDENLAYERLearn about HiddenLayers origin story and what we are all about.AN INTRODUCTION TO AI AND HOW TO PROTECT ITGet a basic understanding of what Articial Intelligence is and the pain pointsthat exist in protecting it.WHAT IS A MACHINE LEARNING MODELDive deeper into what exactly a machine learning model is, and select use casesacross industries.AISEC PLATFORM OVERVIEWReceive a high\-level overview of HiddenLayers AISec Platform. Learn moreabout what the platform provides as well as problems it helps solve.ReadGLOBAL FINANCIAL SERVICES CASE STUDYExplore a tangible customer case study that shows how HiddenLayer helped atop global nancial services company minimize customer experience issueswhile combatting fraud.HiddenLayer ResearchHiddenLayer \& Intel eBook: The Future of Risk is Upon Us
Companies cant adopt a zero\-trust security posture without securing AI. Learnhow to successfully navigate AI adoption and prevent malicious attacks.Forrester Opportunity Snapshot: Its Time for Zero TrustSee how to take charge of AI security condently, stay ahead of threat actors,and enable faster adoption of AI within your products and organization overall.41RESOURCESSecuring AI: A Guide for SecOpsRead this comprehensive overview of the key considerations, risks, and bestpractices that should be taken into account when securing AI deploymentswithin their organizations.The Tactics and Techniques of Adversarial MLDive deeper into the details of adversarial attacks.Weaponizing Machine Learning Modelswith RansomwareSee how easily an adversary can deploy malware through a pre\-trained MLmodel with a destructive impact on an organization.Insane in the Supply ChainUnderstand the scope of your potential exposure through your supply chain riskmanagement, as well as similarly affected technologies involved in machinelearning and their varying levels of risk.The Dark Side of Large Language ModelsLearn more about the perils surrounding the use \- and abuse \- of generative AI.Not So Clear: How MLOps Solutions Can Muddythe Waters of 
Your Supply ChainThis technical report publicly discloses six Zero\-Day vulnerabilities in awell\-known and widely used MLOps platform and demonstrates how thevulnerabilities can be combined to create a full attack chain againstreal\-world systems.Silent Sabotage: Hijacking Safetensors conversion on Hugging FaceLearn how an attacker could compromise the Hugging Face Safetensorsconversion space and its associated service bot.42ABOUT HIDDENLAYERAbout HiddenLayerHiddenLayer, a Gartner recognized AI Application Security company,provides security solutions for articial intelligence algorithms, models, andthe data that power them. With a rst\-of\-its\-kind, non\-invasive softwareapproach to observing and securing AI, HiddenLayer is helping to protectthe worlds most valuable technologies.HiddenLayer was founded by AI professionals and security specialists withrst\-hand experience of how difficult adversarial AI attacks can be to detectand defend against. Determined to prove these attacks are preventable, theteam developed a unique, patent\-pending, productized AI solution to help allorganizations protect important technology.Learn more:Follow us:www.hiddenlayer.comResearchTwitterLinkedInRequest a Demo:https:hiddenlayer.combook\-a\-demoAuthors \& Contributors:A special thank you to the teams that made this report come to life:Marta Janus, Principal Security ResearcherEoin Wickens, Technical Research DirectorTom Bonner, VP of ResearchAndrew Davis, Chief Data ScientistSam Pearcy, GTM SpecialistMalcolm Harkins, Chief Security \& Trust OfficerTravis Smith, VP, ML Threat OperationsChristina Liaghati, PhD, AI Strategy Execution \& Operations Manager at MITRE43